{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\OCRProject.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault('GOOGLE_APPLICATION_CREDENTIALS',r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\OCRProject.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, Variable\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "import json\n",
    "import glob\n",
    "import pytesseract\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"TT-Images\") == False:\n",
    "    os.mkdir(\"TT-Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\COCO\\DIR\\annotations\"\n",
    "train_img_dir = r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\COCO\\DIR\\annotations\"\n",
    "\n",
    "WIDTH = 512\n",
    "HEIGHT = 256\n",
    "category_num = 5 + 1\n",
    "\n",
    "ratio = 8\n",
    "\n",
    "epoch_num = 20\n",
    "batch_size = 4\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UNet(n_channels=3, n_classes=category_num).to(device)\n",
    "net.load_state_dict(torch.load(r'models\\newdata-50-4-0.01.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(location):\n",
    "    img_names = glob.glob(f\"{location}/*.jpg\")\n",
    "    img_names.extend(glob.glob(f\"{location}/*.jpeg\"))\n",
    "    img_names.extend(glob.glob(f\"{location}/*.png\"))\n",
    "    for img_name in img_names:\n",
    "        img = cv2.imread(img_name)\n",
    "        img1 = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "        # HWC -> CHW\n",
    "        img2 = img1.transpose((2, 0, 1))\n",
    "        yield img_name, np.asarray([img2], dtype=np.float32) / 255, img1, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import Output\n",
    "\n",
    "def get_all_ROIs(img):\n",
    "    all_rois = []\n",
    "    d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "    n_boxes = len(d['text'])\n",
    "    for i in range(n_boxes):\n",
    "        if int(d['conf'][i]) > 60:\n",
    "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "            all_rois.append((x,y,x+w,y+h))\n",
    "    return all_rois\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_MICR_Codes(img):\n",
    "    charNames = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\",\"T\", \"U\", \"A\", \"D\"]\n",
    "\n",
    "    ref = cv2.imread(args[\"reference\"])\n",
    "    ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
    "    ref = imutils.resize(ref, width=400)\n",
    "    ref = cv2.threshold(ref, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    refCnts = imutils.grab_contours(refCnts)\n",
    "    refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]\n",
    "    \n",
    "    refROIs = extract_digits_and_symbols(ref, refCnts,minW=10, minH=20)[0]\n",
    "    chars = {}\n",
    "    # loop over the reference ROIs\n",
    "    for (name, roi) in zip(charNames, refROIs):\n",
    "        roi = cv2.resize(roi, (36, 36)) \n",
    "        chars[name] = roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import clear_border\n",
    "from imutils import contours\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "def extract_digits_and_symbols(image, charCnts, minW=5, minH=15):\n",
    "\n",
    "\tcharIter = charCnts.__iter__()\n",
    "\trois = []\n",
    "\tlocs = []\n",
    "\n",
    "\twhile True:\n",
    "\t\ttry:\n",
    "\n",
    "\t\t\tc = next(charIter)\n",
    "\t\t\t(cX, cY, cW, cH) = cv2.boundingRect(c)\n",
    "\t\t\troi = None\n",
    "\n",
    "\t\t\tif cW >= minW and cH >= minH:\n",
    "\t\t\t\t# extract the ROI\n",
    "\t\t\t\troi = image[cY:cY + cH, cX:cX + cW]\n",
    "\t\t\t\trois.append(roi)\n",
    "\t\t\t\tlocs.append((cX, cY, cX + cW, cY + cH))\n",
    "\t\t\telse:\n",
    "\n",
    "\t\t\t\tparts = [c, next(charIter), next(charIter)]\n",
    "\t\t\t\t(sXA, sYA, sXB, sYB) = (np.inf, np.inf, -np.inf,\n",
    "\t\t\t\t\t-np.inf)\n",
    "\t\t\t\t# loop over the parts\n",
    "\t\t\t\tfor p in parts:\n",
    "\t\t\t\t\t# compute the bounding box for the part, then\n",
    "\t\t\t\t\t# update our bookkeeping variables\n",
    "\t\t\t\t\t(pX, pY, pW, pH) = cv2.boundingRect(p)\n",
    "\t\t\t\t\tsXA = min(sXA, pX)\n",
    "\t\t\t\t\tsYA = min(sYA, pY)\n",
    "\t\t\t\t\tsXB = max(sXB, pX + pW)\n",
    "\t\t\t\t\tsYB = max(sYB, pY + pH)\n",
    "\t\t\t\t# extract the ROI\n",
    "\t\t\t\troi = image[sYA:sYB, sXA:sXB]\n",
    "\t\t\t\trois.append(roi)\n",
    "\t\t\t\tlocs.append((sXA, sYA, sXB, sYB))\n",
    "                \n",
    "                \t\t# we have reached the end of the iterator; gracefully break\n",
    "\t\t# from the loop\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tbreak\n",
    "\t# return a tuple of the ROIs and locations\n",
    "\treturn (rois, locs)\n",
    "\n",
    "charNames = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\",\n",
    "\t\"T\", \"U\", \"A\", \"D\"]\n",
    "\n",
    "reference = 'micr_chars.png'\n",
    "\n",
    "ref = cv2.imread(reference)\n",
    "ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
    "ref = imutils.resize(ref, width=400)\n",
    "ref = cv2.threshold(ref, 0, 255, cv2.THRESH_BINARY_INV |\n",
    "\tcv2.THRESH_OTSU)[1]\n",
    "\n",
    "# find contours in the MICR image (i.e,. the outlines of the\n",
    "# characters) and sort them from left to right\n",
    "refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "refCnts = imutils.grab_contours(refCnts)\n",
    "refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]\n",
    "# create a clone of the original image so we can draw on it\n",
    "clone = np.dstack([ref.copy()] * 3)\n",
    "# loop over the (sorted) contours\n",
    "for c in refCnts:\n",
    "\t# compute the bounding box of the contour and draw it on our\n",
    "\t# image\n",
    "\t(x, y, w, h) = cv2.boundingRect(c)\n",
    "\tcv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# extract the digits and symbols from the list of contours, then\n",
    "# initialize a dictionary to map the character name to the ROI\n",
    "(refROIs, refLocs) = extract_digits_and_symbols(ref, refCnts,\n",
    "\tminW=10, minH=20)\n",
    "chars = {}\n",
    "# re-initialize the clone image so we can draw on it again\n",
    "clone = np.dstack([ref.copy()] * 3)\n",
    "# loop over the reference ROIs and locations\n",
    "for (name, roi, loc) in zip(charNames, refROIs, refLocs):\n",
    "\t# draw a bounding box surrounding the character on the output\n",
    "\t# image\n",
    "\t(xA, yA, xB, yB) = loc\n",
    "\tcv2.rectangle(clone, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\t# resize the ROI to a fixed size, then update the characters\n",
    "\t# dictionary, mapping the character name to the ROI\n",
    "\troi = cv2.resize(roi, (36, 36)) \n",
    "\tchars[name] = roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "        print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "    \n",
    "    return texts\n",
    "        \n",
    "def get_google_text(img):\n",
    "    \n",
    "    cv2.imwrite(f\"temp\\\\temp.jpg\",img)\n",
    "    text = detect_text(f\"temp\\\\temp.jpg\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_text(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)\n",
    "    #gray = cv2.medianBlur(gray, 3)\n",
    "    \n",
    "    cv2.imwrite(f\"temp\\\\temp.jpg\",gray)\n",
    "    \n",
    "    config = ('-c tessedit_char_whitelist=/0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-psm 6')\n",
    "    text = pytesseract.image_to_string(Image.open(f\"temp\\\\temp.jpg\"), lang = 'eng',config=config)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def get_MICR_text(img):\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))\n",
    "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "    gradX = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0,\n",
    "\tksize=-1)\n",
    "    gradX = np.absolute(gradX)\n",
    "    (minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
    "    gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))\n",
    "    gradX = gradX.astype(\"uint8\")\n",
    "    gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)\n",
    "    thresh = cv2.threshold(gradX, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    thresh = clear_border(thresh)\n",
    "    groupLocs = get_contours(thresh)\n",
    "    for (gX, gY, gW, gH) in groupLocs:\n",
    "        groupOutput = []\n",
    "\n",
    "        group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\n",
    "        group = cv2.threshold(group, 0, 255,\n",
    "            cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "        cv2.imshow(\"Group\", group)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        charCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE)\n",
    "        charCnts = imutils.grab_contours(charCnts)\n",
    "        charCnts = contours.sort_contours(charCnts,\n",
    "            method=\"left-to-right\")[0]\n",
    "        print (\"extract_digits_and_symbols\")\n",
    "        (rois, locs) = extract_digits_and_symbols(group, charCnts)\n",
    "        \n",
    "    for roi in rois:\n",
    "\n",
    "        scores = []\n",
    "        roi = cv2.resize(roi, (36, 36))\n",
    "        for charName in charNames:\n",
    "            result = cv2.matchTemplate(roi, chars[charName],cv2.TM_CCOEFF)\n",
    "            (_, score, _, _) = cv2.minMaxLoc(result)\n",
    "            scores.append(score)\n",
    "\n",
    "        groupOutput.append(charNames[np.argmax(scores)])\n",
    "    \n",
    "    cv2.rectangle(image, (gX - 10, gY + delta - 10),(gX + gW + 10, gY + gH + delta), (0, 0, 255), 2)\n",
    "    cv2.putText(image, \"\".join(groupOutput),(gX - 10, gY + delta - 25), cv2.FONT_HERSHEY_SIMPLEX,0.95, (0, 0, 255), 3)\n",
    "\n",
    "    output.append(\"\".join(groupOutput))\n",
    "    \n",
    "    # display the output check OCR information to the screen\n",
    "    print(\"Check OCR: {}\".format(\" \".join(output)))\n",
    "    cv2.imshow(\"Check OCR\", image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "def get_contours(gray):\n",
    "    groupCnts = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    groupCnts = imutils.grab_contours(groupCnts)\n",
    "    groupLocs = []\n",
    "    for (i, c) in enumerate(groupCnts):\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        if w > 5 and h > 2:\n",
    "            groupLocs.append((x, y, w, h))\n",
    "    groupLocs = sorted(groupLocs, key=lambda x:x[0])\n",
    "    \n",
    "    return groupLocs\n",
    "\n",
    "def class_x_processing(img,mask_prob,class_id, resized_image, original_image):\n",
    "    ori_x = original_image.shape[1]\n",
    "    ori_y = original_image.shape[0]\n",
    "    Sx = ori_x/abs(ori_x - HEIGHT)\n",
    "    Sy = ori_y/abs(ori_y - WIDTH)\n",
    "    \n",
    "    mask = np.zeros((HEIGHT, WIDTH),np.float32)\n",
    "    indices = np.where(mask_prob==class_id)\n",
    "    mask[indices[0],indices[1]] = 255\n",
    "    curr_time = time.time()\n",
    "    cv2.imwrite(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\TT-Images\\\\{class_id}-mask-{curr_time}.jpg\",mask)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))\n",
    "\n",
    "    gray = cv2.cvtColor(np.repeat(mask[:, :, np.newaxis],3,axis = 2), cv2.COLOR_BGR2GRAY).astype(\"uint8\")\n",
    "    \n",
    "    groupLocs = get_contours(gray)\n",
    "    \n",
    "    for (gX, gY, gW, gH) in groupLocs:\n",
    "        #img = img.transpose((1,2,0))\n",
    "        #print (img.shape)\n",
    "        scaled_gY = int(gY * Sx)\n",
    "        scaled_gH = int(gH * Sx)\n",
    "        scaled_gX = int(gX * Sy)\n",
    "        scaled_gW = int(gW * Sy)\n",
    "        \n",
    "        print (scaled_gY-5,scaled_gY + scaled_gH+5)\n",
    "        group = original_image[scaled_gY-5:scaled_gY + scaled_gH+5, scaled_gX-5:scaled_gX + scaled_gW+5]\n",
    "        print (group.shape)\n",
    "\n",
    "        group = resized_image[gY-5:gY + gH+5, gX-5:gX + gW+5]\n",
    "        cv2.imwrite(f\"test_temp\\\\{curr_time}.jpg\",group)\n",
    "        #rint (gY-5,gY + gH+5, gX-5,gX + gW+5)\n",
    "        #print ((gY + gH+5) - (gY-5), (gX + gW+5) - (gX-5))\n",
    "        if class_id != 4:\n",
    "            continue#text = get_text(group)\n",
    "        else:\n",
    "            text = get_MICR_text(group)\n",
    "        print (text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_possible_ROIs(img):\n",
    "    d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "    n_boxes = len(d['text'])\n",
    "    for i in range(n_boxes):\n",
    "        if int(d['conf'][i]) > 60:\n",
    "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "            img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.imwrite(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\TT-Images\\\\ROI-{time.time()}.jpg\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils,cv2\n",
    "from skimage.segmentation import clear_border\n",
    "from imutils import contours\n",
    "from PIL import Image\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def predict(location):\n",
    "    sub_list = []\n",
    "    net.eval()\n",
    "    count = 0\n",
    "    for img_name, img, resized_image, original_image in test_generator(location):\n",
    "        print (img_name)\n",
    "        X = torch.tensor(img, dtype=torch.float32).to(device)\n",
    "        mask_pred = net(X)\n",
    "        mask_pred1 = mask_pred.cpu().detach().numpy()\n",
    "        mask_prob = np.argmax(mask_pred1, axis=1)\n",
    "        \n",
    "        plot_all_possible_ROIs(original_image)\n",
    "        get_text(original_image)\n",
    "        #all_ROIs = get_all_ROIs(img)\n",
    "        #print (all_ROIs)\n",
    "        \n",
    "        mask_prob = mask_prob.transpose((1, 2, 0))\n",
    "        #print (original_image.shape)\n",
    "        for i in range(5):\n",
    "            class0 = class_x_processing(img,mask_prob,i, resized_image, original_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_img_dir = r\"single_inference\"\n",
    "predict(test_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    \n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
