{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set GOOGLE_APPLICATION_CREDENTIALS=[\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\OCRProject.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, Variable\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "import json\n",
    "import glob\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\COCO\\DIR\\annotations\"\n",
    "train_img_dir = r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\COCO\\DIR\\annotations\"\n",
    "test_img_dir = r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\COCO\\DIR\\annotations\\val\\single_inference\"\n",
    "\n",
    "WIDTH = 256\n",
    "HEIGHT = 128\n",
    "category_num = 5 + 1\n",
    "\n",
    "ratio = 8\n",
    "\n",
    "epoch_num = 20\n",
    "batch_size = 4\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UNet(n_channels=3, n_classes=category_num).to(device)\n",
    "net.load_state_dict(torch.load(r'C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\models\\baseline_newdata.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(location):\n",
    "    img_names = glob.glob(f\"{location}/*.jpg\")\n",
    "    img_names.extend(glob.glob(f\"{location}/*.jpeg\"))\n",
    "    img_names.extend(glob.glob(f\"{location}/*.png\"))\n",
    "    for img_name in img_names:\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "        # HWC -> CHW\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        yield img_name, np.asarray([img], dtype=np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import Output\n",
    "\n",
    "def get_all_ROIs(img):\n",
    "    all_rois = []\n",
    "    d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "    n_boxes = len(d['text'])\n",
    "    for i in range(n_boxes):\n",
    "        if int(d['conf'][i]) > 60:\n",
    "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "            all_rois.append((x,y,x+w,y+h))\n",
    "    return all_rois\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_MICR_Codes(img):\n",
    "    charNames = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\",\"T\", \"U\", \"A\", \"D\"]\n",
    "\n",
    "    ref = cv2.imread(args[\"reference\"])\n",
    "    ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
    "    ref = imutils.resize(ref, width=400)\n",
    "    ref = cv2.threshold(ref, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    refCnts = imutils.grab_contours(refCnts)\n",
    "    refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]\n",
    "    \n",
    "    refROIs = extract_digits_and_symbols(ref, refCnts,minW=10, minH=20)[0]\n",
    "    chars = {}\n",
    "    # loop over the reference ROIs\n",
    "    for (name, roi) in zip(charNames, refROIs):\n",
    "        roi = cv2.resize(roi, (36, 36)) \n",
    "        chars[name] = roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "        print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "        \n",
    "def get_google_text(img):\n",
    "    \n",
    "    cv2.imwrite(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\temp.jpg\",img)\n",
    "    text = detect_text(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\temp.jpg\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_text(img):\n",
    "    \n",
    "    cv2.imwrite(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\temp.jpg\",img)\n",
    "    config = ('-l eng --oem 1 --psm 3')\n",
    "    text = pytesseract.image_to_string(Image.open(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\temp.jpg\"),config=config)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def get_MICR_text(img):\n",
    "    \n",
    "    groupOutput = []\n",
    "    \n",
    "    charCnts = cv2.findContours(img.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    charCnts = imutils.grab_contours(charCnts)\n",
    "    charCnts = contours.sort_contours(charCnts,method=\"left-to-right\")[0]\n",
    "    \n",
    "    (rois, locs) = extract_digits_and_symbols(group, charCnts)\n",
    "\n",
    "    for roi in rois:\n",
    "        scores = []\n",
    "        roi = cv2.resize(roi, (36, 36))\n",
    "        \n",
    "        for charName in charNames:\n",
    "            result = cv2.matchTemplate(roi, chars[charName],cv2.TM_CCOEFF)\n",
    "            (_, score, _, _) = cv2.minMaxLoc(result)\n",
    "            scores.append(score)\n",
    "        groupOutput.append(charNames[np.argmax(scores)])\n",
    "    return \"\".join(groupOutput)\n",
    "\n",
    "def get_contours(gray):\n",
    "    groupCnts = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    groupCnts = imutils.grab_contours(groupCnts)\n",
    "    groupLocs = []\n",
    "\n",
    "    for (i, c) in enumerate(groupCnts):\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        if w > 15 and h > 15:\n",
    "            groupLocs.append((x, y, w, h))\n",
    "    groupLocs = sorted(groupLocs, key=lambda x:x[0])\n",
    "    \n",
    "    return groupLocs\n",
    "\n",
    "def class_x_processing(img,mask_prob,class_id):\n",
    "    mask = np.zeros((HEIGHT, WIDTH),np.float32)\n",
    "    indices = np.where(mask_prob==class_id)\n",
    "    mask[indices[0],indices[1]] = 255\n",
    "    \n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))\n",
    "\n",
    "    gray = cv2.cvtColor(np.repeat(mask[:, :, np.newaxis],3,axis = 2), cv2.COLOR_BGR2GRAY).astype(\"uint8\")\n",
    "    \n",
    "    groupLocs = get_contours(gray)\n",
    "    \n",
    "    cv2.imshow('img',mask)\n",
    "    cv2.waitKey(0)\n",
    "    for (gX, gY, gW, gH) in groupLocs:\n",
    "        \n",
    "        \n",
    "        group = img[gY-5:gY + gH+5, gX-5:gX + gW+5]\n",
    "\n",
    "#         if class_id != 4:\n",
    "#             text = get_text(group)\n",
    "#         else:\n",
    "#             text = get_MICR_text(group)\n",
    "#         print (text)\n",
    "        cv2.imshow('img',group)\n",
    "        cv2.waitKey(0)\n",
    "        text = get_text(group)\n",
    "        print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils,cv2\n",
    "from skimage.segmentation import clear_border\n",
    "from imutils import contours\n",
    "from PIL import Image\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def predict(location):\n",
    "    sub_list = []\n",
    "    net.eval()\n",
    "    count = 0\n",
    "    for img_name, img in test_generator(location):\n",
    "        X = torch.tensor(img, dtype=torch.float32).to(device)\n",
    "        mask_pred = net(X)\n",
    "        mask_pred1 = mask_pred.cpu().detach().numpy()\n",
    "        mask_prob = np.argmax(mask_pred1, axis=1)\n",
    "\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        all_ROIs = get_all_ROIs(img)\n",
    "\n",
    "        mask_prob = mask_prob.transpose((1, 2, 0))\n",
    "\n",
    "        for i in range(5):\n",
    "            class0 = class_x_processing(img,mask_prob,i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    \n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils,cv2\n",
    "from skimage.segmentation import clear_border\n",
    "from imutils import contours\n",
    "from PIL import Image\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "sub_list = []\n",
    "net.eval()\n",
    "count = 0\n",
    "for img_name, img in test_generator(os.path.join(test_img_dir,\"downloaded\")):\n",
    "    X = torch.tensor(img, dtype=torch.float32).to(device)\n",
    "    mask_pred = net(X)\n",
    "    mask_pred1 = mask_pred.cpu().detach().numpy()\n",
    "    mask_prob = np.argmax(mask_pred1, axis=1)\n",
    "    \n",
    "    img = cv2.imread(img_name)\n",
    "    img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    all_ROIs = get_all_ROIs(img)\n",
    "    \n",
    "    mask_prob = mask_prob.transpose((1, 2, 0))\n",
    "    print (np.unique(mask_prob))\n",
    "    \n",
    "    mask = np.zeros((HEIGHT, WIDTH),np.float32)\n",
    "    indices = np.where(mask_prob!=5)\n",
    "    mask[indices[0],indices[1]] = 255\n",
    "\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))\n",
    "\n",
    "    gray = cv2.cvtColor(np.repeat(mask[:, :, np.newaxis],3,axis = 2), cv2.COLOR_BGR2GRAY).astype(\"uint8\")\n",
    "    \n",
    "    \n",
    "    groupCnts = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "    groupCnts = imutils.grab_contours(groupCnts)\n",
    "    groupLocs = []\n",
    "\n",
    "    for (i, c) in enumerate(groupCnts):\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        # only accept the contour region as a grouping of characters if\n",
    "        # the ROI is sufficiently large\n",
    "        if w > 15 and h > 2:\n",
    "            groupLocs.append((x, y, w, h))\n",
    "        \n",
    "#         if w > 15 and h > 15:\n",
    "#             if len(all_ROIs)>0:\n",
    "#                 for bbox in all_ROIs:\n",
    "#                     val = get_iou((x, y, x+w, y+h), bbox)\n",
    "#                     if val>0.1:\n",
    "#                         groupLocs.append(bbox)\n",
    "                        \n",
    "    # sort the digit locations from left-to-right\n",
    "    groupLocs = sorted(groupLocs, key=lambda x:x[0])\n",
    "    \n",
    "    \n",
    "    for (gX, gY, gW, gH) in groupLocs:\n",
    "        \n",
    "        # initialize the group output of characters\n",
    "        groupOutput = []\n",
    "        # extract the group ROI of characters from the grayscale\n",
    "        # image, then apply thresholding to segment the digits from\n",
    "        # the background of the credit card\n",
    "        group = img[gY-5:gY + gH+5, gX-5:gX + gW+5]\n",
    "        group = image_resize(group,height=100)\n",
    "        cv2.imwrite(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\{count}.jpg\",group)\n",
    "        \n",
    "        config = ('-l eng --oem 1 --psm 3')\n",
    "        text = pytesseract.image_to_string(Image.open(f\"C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\{count}.jpg\"),config=config)\n",
    "        print (text,count)\n",
    "        cv2.imshow('img',group)\n",
    "        cv2.waitKey(0)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"var chequeOCR = require('cheque-ocr'),\n",
    "    fs = require('fs');\n",
    "\n",
    "var image = fs.readFileSync(\"\"\"+path\"\"\");\n",
    "chequeOCR(image, function(err, result) {\n",
    "  console.log(err, result);\n",
    "});\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credendtials from environ: C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\OCRProject.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print('Credendtials from environ: {}'.format(os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\masan\\\\Downloads\\\\IDRBT Cheque Image Dataset\\\\OCRProject.json'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.setdefault('GOOGLE_APPLICATION_CREDENTIALS',r\"C:\\Users\\masan\\Downloads\\IDRBT Cheque Image Dataset\\OCRProject.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
